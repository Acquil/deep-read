{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkypeClassifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naR0-HzC_TYf",
        "outputId": "41c7a151-b3e3-4771-989a-230401f80379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!git clone https://github.com/Acquil/deep-read.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-read'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Total 79 (delta 0), reused 0 (delta 0), pack-reused 79\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkheFUOsACN3"
      },
      "source": [
        "!unzip /content/deep-read/data/training/CNN/slides/images_rendered_from_powerpoint.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1o4pV3UAhOF"
      },
      "source": [
        "!unzip \"/content/deep-read/data/training/CNN/video conference screens/video conference screens 1.zip\"\n",
        "!unzip \"/content/deep-read/data/training/CNN/video conference screens/video conference screens 2.zip\"\n",
        "!unzip \"/content/deep-read/data/training/CNN/video conference screens/video conference screens.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAB-F9kMA79l"
      },
      "source": [
        "!mkdir training_data\n",
        "!mv /content/images_rendered_from_powerpoint /content/training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFqBU32hBhS6"
      },
      "source": [
        "!mkdir /content/training_data/screens"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKrQxHN0EUHY"
      },
      "source": [
        "!rm -r /content/training_data/screens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_4LFefHudL"
      },
      "source": [
        "import os\n",
        "folder_list = ['/content/video conference screens/',\n",
        "          '/content/video conference screens 1/',\n",
        "          '/content/video conference screens 2/']\n",
        "\n",
        "for folder in folder_list:\n",
        "  files = os.listdir(folder)\n",
        "  for source in files:\n",
        "    os.rename(folder+source, '/content/training_data/screens/'+source) "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6HXhtXtoBTK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import os\n",
        "from PIL import Image \n",
        "from google.colab import files"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxrGRLrBJOko",
        "outputId": "8b7f6f0c-d56c-43f2-9f43-91b96b63bbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "base_model = ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(299,299,3)\n",
        ")\n",
        "base_model.trainable = False"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3z9MlwJUBv",
        "outputId": "65bde1f9-79ec-4096-8b78-1ab6c3d083e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "imagedatagen = ImageDataGenerator(rescale=1/255,\n",
        "                                  validation_split=0.2\n",
        "                            )\n",
        "train_generator = imagedatagen.flow_from_directory(\n",
        "                            '/content/training_data',\n",
        "                            class_mode=\"binary\",\n",
        "                            target_size=(299,299),\n",
        "                            batch_size=32,\n",
        "                            subset='training')\n",
        "validation_generator = imagedatagen.flow_from_directory(\n",
        "                            '/content/training_data',\n",
        "                            class_mode=\"binary\",\n",
        "                            target_size=(299,299),\n",
        "                            batch_size=32,\n",
        "                            subset='validation')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1821 images belonging to 2 classes.\n",
            "Found 455 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfy_X_fHJXvc"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(299, 299, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(1024,activation='relu')(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dropout(0.1)(x)\n",
        "x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, x)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK_zTDWFJeAw"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch < 20:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "class mycallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self,epoch,logs={}):\n",
        "            if(logs['val_accuracy']>=0.998):\n",
        "                self.model.stop_training=True"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_piG2rVJgpO"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy',tf.keras.metrics.Precision(),\n",
        "                        tf.keras.metrics.Recall()]\n",
        "              )"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ftzaf-2JlFt",
        "outputId": "7f2f7074-c933-4cb9-a243-a0fe9cd196dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "tr_callback = mycallback()\n",
        "\n",
        "model.fit(train_generator,\n",
        "          epochs=30,\n",
        "          validation_data = validation_generator,\n",
        "          callbacks=[lr_callback,tr_callback])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 2/57 [>.............................] - ETA: 9s - loss: 8.5429e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 30s 524ms/step - loss: 0.0708 - accuracy: 0.9769 - precision: 0.9707 - recall: 0.9707 - val_loss: 0.1054 - val_accuracy: 0.9714 - val_precision: 0.9560 - val_recall: 0.9721\n",
            "Epoch 2/30\n",
            "57/57 [==============================] - 30s 528ms/step - loss: 0.0420 - accuracy: 0.9819 - precision: 0.9777 - recall: 0.9763 - val_loss: 0.1857 - val_accuracy: 0.9516 - val_precision: 0.9937 - val_recall: 0.8827\n",
            "Epoch 3/30\n",
            "57/57 [==============================] - 30s 531ms/step - loss: 0.0401 - accuracy: 0.9835 - precision: 0.9818 - recall: 0.9763 - val_loss: 0.1923 - val_accuracy: 0.9538 - val_precision: 0.9817 - val_recall: 0.8994\n",
            "Epoch 4/30\n",
            "57/57 [==============================] - 30s 530ms/step - loss: 0.0497 - accuracy: 0.9824 - precision: 0.9804 - recall: 0.9749 - val_loss: 0.1474 - val_accuracy: 0.9516 - val_precision: 0.9067 - val_recall: 0.9777\n",
            "Epoch 5/30\n",
            "57/57 [==============================] - 30s 528ms/step - loss: 0.0191 - accuracy: 0.9945 - precision: 0.9944 - recall: 0.9916 - val_loss: 0.1259 - val_accuracy: 0.9560 - val_precision: 0.9162 - val_recall: 0.9777\n",
            "Epoch 6/30\n",
            "57/57 [==============================] - 30s 532ms/step - loss: 0.0231 - accuracy: 0.9907 - precision: 0.9861 - recall: 0.9902 - val_loss: 0.1663 - val_accuracy: 0.9582 - val_precision: 0.9819 - val_recall: 0.9106\n",
            "Epoch 7/30\n",
            "57/57 [==============================] - 30s 530ms/step - loss: 0.0443 - accuracy: 0.9874 - precision: 0.9860 - recall: 0.9819 - val_loss: 0.4547 - val_accuracy: 0.8923 - val_precision: 1.0000 - val_recall: 0.7263\n",
            "Epoch 8/30\n",
            "57/57 [==============================] - 30s 533ms/step - loss: 0.0186 - accuracy: 0.9929 - precision: 0.9903 - recall: 0.9916 - val_loss: 0.1602 - val_accuracy: 0.9560 - val_precision: 0.9938 - val_recall: 0.8939\n",
            "Epoch 9/30\n",
            "57/57 [==============================] - 31s 535ms/step - loss: 0.0247 - accuracy: 0.9901 - precision: 0.9874 - recall: 0.9874 - val_loss: 0.2060 - val_accuracy: 0.9451 - val_precision: 0.9936 - val_recall: 0.8659\n",
            "Epoch 10/30\n",
            "57/57 [==============================] - 31s 540ms/step - loss: 0.0218 - accuracy: 0.9918 - precision: 0.9889 - recall: 0.9902 - val_loss: 0.0990 - val_accuracy: 0.9714 - val_precision: 0.9663 - val_recall: 0.9609\n",
            "Epoch 11/30\n",
            "57/57 [==============================] - 31s 541ms/step - loss: 0.0308 - accuracy: 0.9907 - precision: 0.9875 - recall: 0.9888 - val_loss: 0.2525 - val_accuracy: 0.9407 - val_precision: 0.8838 - val_recall: 0.9777\n",
            "Epoch 12/30\n",
            "57/57 [==============================] - 31s 539ms/step - loss: 0.0107 - accuracy: 0.9945 - precision: 0.9930 - recall: 0.9930 - val_loss: 0.1122 - val_accuracy: 0.9692 - val_precision: 0.9940 - val_recall: 0.9274\n",
            "Epoch 13/30\n",
            "57/57 [==============================] - 30s 533ms/step - loss: 0.0129 - accuracy: 0.9962 - precision: 0.9958 - recall: 0.9944 - val_loss: 0.3884 - val_accuracy: 0.9231 - val_precision: 0.9675 - val_recall: 0.8324\n",
            "Epoch 14/30\n",
            "57/57 [==============================] - 30s 528ms/step - loss: 0.0270 - accuracy: 0.9890 - precision: 0.9847 - recall: 0.9874 - val_loss: 0.4738 - val_accuracy: 0.9077 - val_precision: 1.0000 - val_recall: 0.7654\n",
            "Epoch 15/30\n",
            "57/57 [==============================] - 30s 526ms/step - loss: 0.0161 - accuracy: 0.9934 - precision: 0.9930 - recall: 0.9902 - val_loss: 0.2344 - val_accuracy: 0.9407 - val_precision: 0.9872 - val_recall: 0.8603\n",
            "Epoch 16/30\n",
            "57/57 [==============================] - 30s 531ms/step - loss: 0.0060 - accuracy: 0.9984 - precision: 0.9972 - recall: 0.9986 - val_loss: 0.1775 - val_accuracy: 0.9451 - val_precision: 0.9326 - val_recall: 0.9274\n",
            "Epoch 17/30\n",
            "57/57 [==============================] - 30s 525ms/step - loss: 0.0051 - accuracy: 0.9978 - precision: 0.9972 - recall: 0.9972 - val_loss: 0.1345 - val_accuracy: 0.9473 - val_precision: 0.9429 - val_recall: 0.9218\n",
            "Epoch 18/30\n",
            "57/57 [==============================] - 30s 526ms/step - loss: 0.0279 - accuracy: 0.9907 - precision: 0.9888 - recall: 0.9874 - val_loss: 0.2067 - val_accuracy: 0.9451 - val_precision: 0.9583 - val_recall: 0.8994\n",
            "Epoch 19/30\n",
            "57/57 [==============================] - 30s 523ms/step - loss: 0.0401 - accuracy: 0.9885 - precision: 0.9874 - recall: 0.9833 - val_loss: 0.2964 - val_accuracy: 0.9385 - val_precision: 0.9748 - val_recall: 0.8659\n",
            "Epoch 20/30\n",
            "57/57 [==============================] - 30s 519ms/step - loss: 0.0227 - accuracy: 0.9940 - precision: 0.9958 - recall: 0.9888 - val_loss: 0.6955 - val_accuracy: 0.8857 - val_precision: 0.9205 - val_recall: 0.7765\n",
            "Epoch 21/30\n",
            "57/57 [==============================] - 30s 523ms/step - loss: 0.0211 - accuracy: 0.9945 - precision: 0.9944 - recall: 0.9916 - val_loss: 0.2178 - val_accuracy: 0.9407 - val_precision: 0.9368 - val_recall: 0.9106\n",
            "Epoch 22/30\n",
            "57/57 [==============================] - 30s 519ms/step - loss: 0.0108 - accuracy: 0.9962 - precision: 0.9931 - recall: 0.9972 - val_loss: 0.2202 - val_accuracy: 0.9407 - val_precision: 0.9419 - val_recall: 0.9050\n",
            "Epoch 23/30\n",
            "57/57 [==============================] - 30s 523ms/step - loss: 0.0164 - accuracy: 0.9940 - precision: 0.9930 - recall: 0.9916 - val_loss: 0.4282 - val_accuracy: 0.8923 - val_precision: 0.7902 - val_recall: 0.9888\n",
            "Epoch 24/30\n",
            "57/57 [==============================] - 30s 525ms/step - loss: 0.0118 - accuracy: 0.9945 - precision: 0.9944 - recall: 0.9916 - val_loss: 0.1667 - val_accuracy: 0.9341 - val_precision: 0.8782 - val_recall: 0.9665\n",
            "Epoch 25/30\n",
            "57/57 [==============================] - 30s 518ms/step - loss: 0.0060 - accuracy: 0.9984 - precision: 0.9986 - recall: 0.9972 - val_loss: 0.1461 - val_accuracy: 0.9538 - val_precision: 0.9702 - val_recall: 0.9106\n",
            "Epoch 26/30\n",
            "57/57 [==============================] - 30s 521ms/step - loss: 0.0056 - accuracy: 0.9978 - precision: 0.9972 - recall: 0.9972 - val_loss: 0.1341 - val_accuracy: 0.9538 - val_precision: 0.9341 - val_recall: 0.9497\n",
            "Epoch 27/30\n",
            "57/57 [==============================] - 30s 525ms/step - loss: 0.0019 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9495 - val_precision: 0.9382 - val_recall: 0.9330\n",
            "Epoch 28/30\n",
            "57/57 [==============================] - 30s 523ms/step - loss: 0.0072 - accuracy: 0.9967 - precision: 0.9958 - recall: 0.9958 - val_loss: 0.1371 - val_accuracy: 0.9473 - val_precision: 0.9379 - val_recall: 0.9274\n",
            "Epoch 29/30\n",
            "57/57 [==============================] - 30s 523ms/step - loss: 0.0037 - accuracy: 0.9989 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.1529 - val_accuracy: 0.9560 - val_precision: 0.9595 - val_recall: 0.9274\n",
            "Epoch 30/30\n",
            "57/57 [==============================] - 30s 526ms/step - loss: 0.0065 - accuracy: 0.9978 - precision: 0.9958 - recall: 0.9986 - val_loss: 0.4084 - val_accuracy: 0.9165 - val_precision: 1.0000 - val_recall: 0.7877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a48a24208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXRF5VBoJpS_",
        "outputId": "97e66c66-91e3-481c-8f22-512411de8487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "model.save(\"model_v1_resnet50\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: model_v1_renet50/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nNMkvW2vcAX"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-9ooefyoV7r"
      },
      "source": [
        "model = tf.keras.models.load_model('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h5LxorRmwEd",
        "outputId": "784cd9d7-e2a3-4004-ffeb-d8ac23ea7b01",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "uploaded = files.upload()\n",
        "for img_path in uploaded.keys():\n",
        "  img = load_img(img_path, target_size=(299, 299))  # this is a PIL image\n",
        "  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "  # Rescale by 1/255\n",
        "  x /= 255\n",
        "\n",
        "  class_ = model.predict(x)\n",
        "  if class_[0][0] >0.5:\n",
        "    print(\"Other\")\n",
        "  else:\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5dc70e17-276c-4956-8135-2d95d9930558\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5dc70e17-276c-4956-8135-2d95d9930558\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Screenshot (64).png to Screenshot (64).png\n",
            "Other\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3TcXzgsoPX3",
        "outputId": "a0d4f175-a0d6-4dda-ff4a-81ea1617cff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "imagedatagen = ImageDataGenerator(rescale=1/255,\n",
        "                                  validation_split=0.2,\n",
        "                            )\n",
        "datagen = imagedatagen.flow_from_directory(\n",
        "                            '/content/data',\n",
        "                            class_mode=\"binary\",\n",
        "                            target_size=(299,299),\n",
        "                            batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 61 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p66f4Giis-ls"
      },
      "source": [
        "folder = ''\n",
        "files = os.listdir(folder)\n",
        "\n",
        "for img_path in  files:\n",
        "  if img_path == '.ipynb_checkpoints' or img_path == 'desktop.ini':\n",
        "    continue\n",
        "  print(img_path)\n",
        "  img = load_img(folder+img_path, target_size=(299, 299))  # this is a PIL image\n",
        "  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "  # Rescale by 1/255\n",
        "  x /= 255\n",
        "\n",
        "  class_ = model.predict(x)\n",
        "  if class_[0][0] >0.5:\n",
        "    print(\"Other\")\n",
        "  else:\n",
        "    print(\"skype\")\n",
        "    # creating a object"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}